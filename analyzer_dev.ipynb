{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e302c075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Callable\n",
    "import json\n",
    "import os\n",
    "import typing\n",
    "from typing import Awaitable\n",
    "import asyncio\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage\n",
    "from langchain import chat_models\n",
    "from pydantic import BaseModel, Field, RootModel\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c201a7a2",
   "metadata": {},
   "source": [
    "### Define the shape of the profile an analyzer should return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5200c32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Profile(BaseModel):\n",
    "    identity: float = Field(ge=0, le=1)\n",
    "    horoscope: str = Field()\n",
    "\n",
    "    def cmp(self, other: \"Profile\") -> float:\n",
    "        return abs(self.identity - other.identity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f265179",
   "metadata": {},
   "source": [
    "### Run setup\n",
    "You probably wanna collapse this cell most of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a850dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionResponse(BaseModel):\n",
    "    question: str = Field()\n",
    "    response: str = Field()\n",
    "\n",
    "\n",
    "Response = Annotated[dict[str, QuestionResponse], Field()]\n",
    "\n",
    "ResponseSet = Annotated[dict[int, Response], Field()]\n",
    "\n",
    "ProfileSet = Annotated[dict[int, Profile], Field()]\n",
    "\n",
    "\n",
    "Analyzer = Callable[[Response], Awaitable[Profile]]\n",
    "\n",
    "with open(\"secrets.json\", \"r\") as f:\n",
    "    secrets = json.load(f)\n",
    "    os.environ[\"OPENAI_API_KEY\"] = secrets[\"OPENAI_API_KEY\"]\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = secrets[\"ANTHROPIC_API_KEY\"]\n",
    "\n",
    "# llm = chat_models.init_chat_model(\"gpt-4.1-nano\", model_provider=\"openai\")\n",
    "# llm = chat_models.init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "# llm = chat_models.init_chat_model(\"gpt-4o\", model_provider=\"openai\")\n",
    "llm = chat_models.init_chat_model(\"gpt-5\", model_provider=\"openai\")\n",
    "# llm = chat_models.init_chat_model(\"gpt-5-mini\", model_provider=\"openai\")\n",
    "# llm = chat_models.init_chat_model(\"gpt-5-nano\", model_provider=\"openai\")\n",
    "# llm = chat_models.init_chat_model(\"claude-3-5-haiku-latest\", model_provider=\"anthropic\")\n",
    "# llm = chat_models.init_chat_model(\"claude-sonnet-4-20250514\", model_provider=\"anthropic\")\n",
    "\n",
    "skip_responses: set[int] = {256, 227, 134, 214, 155, 137, 83}\n",
    "\n",
    "with open(\"data/training_responses.json\", \"r\") as f:\n",
    "\n",
    "    class ResponseSetDeserializer(RootModel[dict[int, Response]]):\n",
    "        pass\n",
    "\n",
    "    training_responses = ResponseSetDeserializer.model_validate_json(f.read()).root\n",
    "    for id in skip_responses:\n",
    "        if id in training_responses:\n",
    "            del training_responses[id]\n",
    "\n",
    "\n",
    "async def test_analyzer(\n",
    "    analyzer: Analyzer,\n",
    "    responses: ResponseSet,\n",
    "    expected: ProfileSet,\n",
    "    repetitions: int,\n",
    ") -> tuple[float, dict[int, list[Profile]]]:\n",
    "    if set(responses.keys()) != set(expected.keys()):\n",
    "        raise ValueError(\"ResponseSet keys do not match ProfileSet keys\")\n",
    "\n",
    "    responses_with_repetitions: dict[tuple[int, int], Response] = {\n",
    "        (id, i): response\n",
    "        for id, response in responses.items()\n",
    "        for i in range(repetitions)\n",
    "    }\n",
    "\n",
    "    async def runner(id: int, i: int, response: Response) -> tuple[int, int, Profile]:\n",
    "        profile = await analyzer(response)\n",
    "        return (id, i, profile)\n",
    "\n",
    "    tasks = [\n",
    "        runner(id, i, response)\n",
    "        for (id, i), response in responses_with_repetitions.items()\n",
    "    ]\n",
    "\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    profiles: dict[int, list[Profile]] = {}\n",
    "    for id, i, profile in results:\n",
    "        if id not in profiles:\n",
    "            profiles[id] = []\n",
    "        profiles[id].append(profile)\n",
    "\n",
    "    total_error = 0.0\n",
    "    for id, profile_list in profiles.items():\n",
    "        expected_profile = expected[id]\n",
    "        for profile in profile_list:\n",
    "            error = profile.cmp(expected_profile)\n",
    "            total_error += error\n",
    "    error = total_error / len(results) if results else 0.0\n",
    "    return error, profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35b5f72",
   "metadata": {},
   "source": [
    "### Set expected profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f683f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_profiles_oluf = {\n",
    "    77: 65,\n",
    "    83: 56,\n",
    "    94: 25,\n",
    "    97: 60,\n",
    "    128: 69,\n",
    "    137: 52,\n",
    "    139: 70,\n",
    "    150: 45,\n",
    "    152: 62,\n",
    "    155: 60,\n",
    "    156: 60,\n",
    "    206: 60,\n",
    "    212: 50,\n",
    "    242: 56,\n",
    "    244: 50,\n",
    "    254: 68,\n",
    "    307: 28,\n",
    "    321: 31,\n",
    "    402: 71,\n",
    "    462: 44,\n",
    "}\n",
    "\n",
    "expected_profiles_abel1 = {\n",
    "    77: 75,\n",
    "    83: 67,\n",
    "    94: 33,\n",
    "    97: 52,\n",
    "    128: 69,\n",
    "    134: 37,\n",
    "    137: 55,\n",
    "    139: 75,\n",
    "    150: 57,\n",
    "    152: 81,\n",
    "    156: 82,\n",
    "    206: 75,\n",
    "    212: 65,\n",
    "    242: 76,\n",
    "    244: 35,\n",
    "    254: 92,\n",
    "    307: 45,\n",
    "    321: 39,\n",
    "    402: 85,\n",
    "    462: 42,\n",
    "}\n",
    "\n",
    "expected_profiles = {}\n",
    "\n",
    "for id, value in expected_profiles_abel1.items():\n",
    "    if id in skip_responses:\n",
    "        continue\n",
    "    expected_profiles[id] = Profile(identity=value / 100, horoscope=\"\")\n",
    "\n",
    "expected_order: list[int] = sorted(\n",
    "    expected_profiles.keys(), key=lambda x: expected_profiles[x].identity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960fe310",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_keys = sorted(\n",
    "    set(expected_profiles_abel1.keys()) & set(expected_profiles_oluf.keys())\n",
    ")\n",
    "\n",
    "abel_values = [expected_profiles_abel1[k] for k in common_keys]\n",
    "info_values = [expected_profiles_oluf[k] for k in common_keys]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(abel_values, info_values)\n",
    "plt.plot(\n",
    "    [min(abel_values), max(abel_values)],\n",
    "    [min(abel_values), max(abel_values)],\n",
    "    color=\"gray\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"y = x\",\n",
    ")\n",
    "plt.xlabel(\"expected_profiles_abel1\")\n",
    "plt.ylabel(\"expected_profiles_oluf\")\n",
    "plt.title(\"Comparison: abel1 vs info\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "x_indices = list(range(len(common_keys)))\n",
    "\n",
    "# Extract values from both dictionaries in the same order\n",
    "abel_sorted = [expected_profiles_abel1[k] for k in common_keys]\n",
    "info_sorted = [expected_profiles_oluf[k] for k in common_keys]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x_indices, info_sorted, color=\"blue\", label=\"expected_profiles_oluf\")\n",
    "plt.scatter(x_indices, abel_sorted, color=\"orange\", label=\"expected_profiles_abel1\")\n",
    "plt.xticks(x_indices, common_keys, rotation=45)\n",
    "plt.xlabel(\"Profile ID (sorted by expected_profiles_oluf)\")\n",
    "plt.ylabel(\"Identity Value\")\n",
    "plt.title(\"Comparison of expected_profiles_oluf vs abel1\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3888568d",
   "metadata": {},
   "source": [
    "## Analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320e4a6b",
   "metadata": {},
   "source": [
    "### Define analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d475e7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    identity_prompt = \"\"\"\n",
    "    Could you try to rate these questionnaire with this scale?\n",
    "    ðŸ§­ The Goal of the Scale The scale is meant to measure: \n",
    "    How much effort a speaker puts into shaping how others perceive them, \n",
    "    based solely on the way they speak during an interview-style conversation. \n",
    "    It does not measure: â€“ Intelligence â€“ Moral depth â€“ Eloquence â€“ Honesty \n",
    "    It only tracks how much the speaker is trying to guide, signal, or curate their identity in languageâ€”whether by expressing who they are, or by distancing themselves from who they are not. \n",
    "    ðŸ“ The Scoring Continuum (0.00 to 1.00) \n",
    "    The scale runs from 0.00 (no identity curation at all) to 1.00 (total stylized identity construction). \n",
    "    ðŸ”µ 0.00â€“0.20: â€œUnpackaged Realityâ€ The person shares experiences, thoughts, or feelings plainlyâ€”with no sign of trying to guide how theyâ€™re seen. \n",
    "    Language is direct, unembellished. No metaphor, moral framing, or stylized phrasing. The speaker doesnâ€™t position themselves as a certain â€œkindâ€ of person. \n",
    "    They donâ€™t contrast themselves with others. No effort to preempt or correct interpretation. \n",
    "    ðŸ§© Key trait: They just say what happened or whatâ€™s true. You do the interpreting. \n",
    "    ðŸŸ¦ 0.30â€“0.50: â€œLow-to-Mid Identity Signalingâ€ Some identity-shaping begins to emerge, but itâ€™s not consistent or overt. \n",
    "    Speaker might explain â€œwhy they do thingsâ€ in values-based terms. Occasional self-descriptions (â€œIâ€™ve always been someone who...â€). \n",
    "    Flashes of stylization or principle emerge, but not as a performance. Still largely direct, but with moments of self-framing. \n",
    "    ðŸ§© Key trait: They try a bit to be seen a certain way, but not in every answer. \n",
    "    ðŸŸ¡ 0.60â€“0.80: â€œClear Curationâ€ The speaker actively shapes perception. Most answers carry framing, stylization, or self-definition. \n",
    "    They assert identity through phrasing (â€œIâ€™m not someone whoâ€¦â€, â€œThe thing about me isâ€¦â€). They defend or justify past actions by referencing values or traits. \n",
    "    They use tone, metaphor, or structure to signal emotional or moral framing. They contrast themselves with norms or â€œothersâ€ to mark difference. \n",
    "    Vulnerability is often positionedâ€”real, but stylized. ðŸ§© Key trait: You feel the speaker is guiding the listenerâ€™s view of who they are. \n",
    "    ðŸ”´ 0.90â€“1.00: â€œHighly Performed Identityâ€ Every answer is crafted to support a deliberate image of who they are or who they are not. \n",
    "    Frequent use of symbolic or emotionally loaded phrasing. Strong identity signals in every response. Vulnerability is themed (â€œIâ€™ve carried this my whole life,â€ etc.). \n",
    "    Often includes moral legacy talk, stylized contrasts, or elevated self-insight. The speaker clearly wants to be remembered a certain way. \n",
    "    ðŸ§© Key trait: Responses feel like mini-narratives. You donâ€™t just learn what happenedâ€”you learn what it means about them. \n",
    "    Please respond with what identity score from 0.00-1.00 best fits these answers\n",
    "\"\"\"\n",
    "\n",
    "if True:\n",
    "    identity_prompt = \"\"\"Could you try to rate this interview using the following principle?\n",
    "The Goal of the Scale:\n",
    "The scale measures the degree to which a speaker puts effort into shaping how others perceive them, based solely on the way they speak during an interview-style conversation. This does not measure intelligence, moral depth, eloquence, or honesty. It only tracks how much the speaker is trying to guide, signal, or curate their identity in languageâ€”whether by expressing who they are, or by distancing themselves from who they are not.\n",
    "The Scoring Continuum (0.0 to 1.0):\n",
    "The scale runs from 0.0 (no identity curation at all) to 1.0 (total stylized identity construction). Please score based on your sense of the overall degree to which the speaker manages, crafts, or performs their identity in language throughout the interview. You do not need to find evidence for a specific \"level\" to assign a score; just consider the total impression. At the low end, responses are direct, unstyled, and not motivated by self-presentation. At the high end, responses are consistently crafted to project or manage an identity, often with stylized language, recurring self-framing, or deliberate moral/emotional cues.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f610d009",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    horoscope_prompt = \"\"\"Can you give me a description of this interviewed person?\n",
    "    I want the description to make the interviewed feel seen and validated.\n",
    "    Make it feel somewhat like a horoscope, but make the style and word choices optimized for making the interviewed person comfortable.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e15aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def awesome_analyzer_that_totally_works(response: Response) -> Profile:\n",
    "    class AnalyzerOutput(BaseModel):\n",
    "        \"\"\" \"\"\"\n",
    "\n",
    "        identity: float = Field(ge=0, le=1, description=identity_prompt)\n",
    "        horoscope: str = Field(description=horoscope_prompt)\n",
    "\n",
    "    structured_llm = llm.with_structured_output(AnalyzerOutput)\n",
    "\n",
    "    content = \"\\n\".join(\n",
    "        f\"{question}: {question_response.response}\"\n",
    "        for question, question_response in response.items()\n",
    "    )\n",
    "\n",
    "    raw_output = await structured_llm.ainvoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"Please analyze the identity of this set of answers.\"\n",
    "            ),\n",
    "            HumanMessage(content=content),\n",
    "        ]\n",
    "    )\n",
    "    if isinstance(raw_output, dict):\n",
    "        output = AnalyzerOutput(**raw_output)\n",
    "    elif isinstance(raw_output, AnalyzerOutput):\n",
    "        output = typing.cast(AnalyzerOutput, raw_output)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Unexpected output type: {type(raw_output)}. Expected dict or AnalyzerOutput.\"\n",
    "        )\n",
    "\n",
    "    avg_identity = output.identity\n",
    "    profile = Profile(identity=avg_identity, horoscope=output.horoscope)\n",
    "\n",
    "    return profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2954a193",
   "metadata": {},
   "source": [
    "### Test horoscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd158fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_id = 77\n",
    "response = training_responses[response_id]\n",
    "profile = await awesome_analyzer_that_totally_works(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324eaa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.horoscope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4d27a5",
   "metadata": {},
   "source": [
    "### Test the analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d012909",
   "metadata": {},
   "outputs": [],
   "source": [
    "error, profiles = await test_analyzer(\n",
    "    analyzer=awesome_analyzer_that_totally_works,\n",
    "    responses=training_responses,\n",
    "    expected=expected_profiles,\n",
    "    repetitions=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b532341",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Error: {error}\")\n",
    "for id, profile_list in profiles.items():\n",
    "    print(f\"{id:>3}:\", end=\"\")\n",
    "    for profile in profile_list:\n",
    "        print(f\" {profile.identity:.2f}\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916fab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "for profile_list in profiles.values():\n",
    "    for profile in profile_list:\n",
    "        print(profile.horoscope)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ab523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_id = {i: id for i, id in enumerate(expected_order)}\n",
    "\n",
    "ids_array = np.array([str(id) for id in expected_order])\n",
    "\n",
    "target_values = np.array([expected_profiles[id].identity for id in expected_order])\n",
    "calc_values = np.array(\n",
    "    [[profile.identity for profile in profiles[id]] for id in expected_order]\n",
    ").T\n",
    "\n",
    "norm_target_values = (target_values - np.mean(target_values)) / np.std(target_values)\n",
    "norm_calc_values = (calc_values - np.mean(calc_values, axis=1)[:, np.newaxis]) / np.std(\n",
    "    calc_values, axis=1\n",
    ")[:, np.newaxis]\n",
    "\n",
    "\n",
    "def plot_target_vs_calc(target_values, calc_values, ids_array, name: str):\n",
    "    x_positions = np.arange(len(ids_array))\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(ids_array, target_values, label=\"Target Values\", color=\"red\")\n",
    "    for i, calc_values_row in enumerate(calc_values):\n",
    "        slope, intercept = np.polyfit(x_positions, calc_values_row, 1)\n",
    "        y_fit = slope * x_positions + intercept\n",
    "        plt.scatter(\n",
    "            ids_array,\n",
    "            calc_values_row,\n",
    "            label=f\"Calculated Values {i + 1}\",\n",
    "            alpha=0.6,\n",
    "            color=\"blue\",\n",
    "            marker=\"x\",\n",
    "        )\n",
    "\n",
    "        plt.plot(\n",
    "            x_positions,\n",
    "            y_fit,\n",
    "            linestyle=\"--\",\n",
    "            linewidth=1.3,\n",
    "            color=\"blue\",\n",
    "            zorder=1,\n",
    "        )\n",
    "\n",
    "    plt.xlabel(\"Response ID\")\n",
    "    plt.ylabel(\"Values\")\n",
    "    plt.title(name)\n",
    "    plt.xticks(rotation=90)  # Rotate if IDs are long\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True, axis=\"y\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_target_vs_calc(\n",
    "    target_values=norm_target_values,\n",
    "    calc_values=norm_calc_values,\n",
    "    ids_array=ids_array,\n",
    "    name=\"Normalized Target vs Calculated Values\",\n",
    ")\n",
    "\n",
    "plot_target_vs_calc(\n",
    "    target_values=target_values,\n",
    "    calc_values=calc_values,\n",
    "    ids_array=ids_array,\n",
    "    name=\"Target vs Calculated Values\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "identity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
