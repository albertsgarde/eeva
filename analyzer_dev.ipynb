{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e302c075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Callable\n",
    "import json\n",
    "import os\n",
    "import typing\n",
    "from typing import Awaitable\n",
    "import asyncio\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage\n",
    "from langchain import chat_models\n",
    "from pydantic import BaseModel, Field, RootModel\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c201a7a2",
   "metadata": {},
   "source": [
    "### Define the shape of the profile an analyzer should return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5200c32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Profile(BaseModel):\n",
    "    identity: float = Field(ge=0, le=1)\n",
    "    horoscope: str = Field()\n",
    "\n",
    "    def cmp(self, other: \"Profile\") -> float:\n",
    "        return abs(self.identity - other.identity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f265179",
   "metadata": {},
   "source": [
    "### Run setup\n",
    "You probably wanna collapse this cell most of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a850dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionResponse(BaseModel):\n",
    "    question: str = Field()\n",
    "    response: str = Field()\n",
    "\n",
    "\n",
    "Response = Annotated[dict[str, QuestionResponse], Field()]\n",
    "\n",
    "ResponseSet = Annotated[dict[int, Response], Field()]\n",
    "\n",
    "ProfileSet = Annotated[dict[int, Profile], Field()]\n",
    "\n",
    "\n",
    "Analyzer = Callable[[Response], Awaitable[Profile]]\n",
    "\n",
    "with open(\"secrets.json\", \"r\") as f:\n",
    "    secrets = json.load(f)\n",
    "    os.environ[\"OPENAI_API_KEY\"] = secrets[\"OPENAI_API_KEY\"]\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = secrets[\"ANTHROPIC_API_KEY\"]\n",
    "\n",
    "# llm = chat_models.init_chat_model(\"gpt-4.1-nano\", model_provider=\"openai\")\n",
    "# llm = chat_models.init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "# llm = chat_models.init_chat_model(\"gpt-4o\", model_provider=\"openai\")\n",
    "llm = chat_models.init_chat_model(\"gpt-5\", model_provider=\"openai\")\n",
    "# llm = chat_models.init_chat_model(\"gpt-5-mini\", model_provider=\"openai\")\n",
    "# llm = chat_models.init_chat_model(\"gpt-5-nano\", model_provider=\"openai\")\n",
    "# llm = chat_models.init_chat_model(\"claude-3-5-haiku-latest\", model_provider=\"anthropic\")\n",
    "# llm = chat_models.init_chat_model(\"claude-sonnet-4-20250514\", model_provider=\"anthropic\")\n",
    "\n",
    "skip_responses: set[int] = {256, 227, 134, 214, 155, 137, 83}\n",
    "\n",
    "with open(\"data/training_responses.json\", \"r\") as f:\n",
    "\n",
    "    class ResponseSetDeserializer(RootModel[dict[int, Response]]):\n",
    "        pass\n",
    "\n",
    "    training_responses = ResponseSetDeserializer.model_validate_json(f.read()).root\n",
    "    for id in skip_responses:\n",
    "        if id in training_responses:\n",
    "            del training_responses[id]\n",
    "\n",
    "\n",
    "async def test_analyzer(\n",
    "    analyzer: Analyzer,\n",
    "    responses: ResponseSet,\n",
    "    expected: ProfileSet,\n",
    "    repetitions: int,\n",
    ") -> tuple[float, dict[int, list[Profile]]]:\n",
    "    if set(responses.keys()) != set(expected.keys()):\n",
    "        raise ValueError(\"ResponseSet keys do not match ProfileSet keys\")\n",
    "\n",
    "    responses_with_repetitions: dict[tuple[int, int], Response] = {\n",
    "        (id, i): response\n",
    "        for id, response in responses.items()\n",
    "        for i in range(repetitions)\n",
    "    }\n",
    "\n",
    "    async def runner(id: int, i: int, response: Response) -> tuple[int, int, Profile]:\n",
    "        profile = await analyzer(response)\n",
    "        return (id, i, profile)\n",
    "\n",
    "    tasks = [\n",
    "        runner(id, i, response)\n",
    "        for (id, i), response in responses_with_repetitions.items()\n",
    "    ]\n",
    "\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    profiles: dict[int, list[Profile]] = {}\n",
    "    for id, i, profile in results:\n",
    "        if id not in profiles:\n",
    "            profiles[id] = []\n",
    "        profiles[id].append(profile)\n",
    "\n",
    "    total_error = 0.0\n",
    "    for id, profile_list in profiles.items():\n",
    "        expected_profile = expected[id]\n",
    "        for profile in profile_list:\n",
    "            error = profile.cmp(expected_profile)\n",
    "            total_error += error\n",
    "    error = total_error / len(results) if results else 0.0\n",
    "    return error, profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35b5f72",
   "metadata": {},
   "source": [
    "### Set expected profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f683f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_profiles_oluf = {\n",
    "    77: 65,\n",
    "    83: 56,\n",
    "    94: 25,\n",
    "    97: 60,\n",
    "    128: 69,\n",
    "    137: 52,\n",
    "    139: 70,\n",
    "    150: 45,\n",
    "    152: 62,\n",
    "    155: 60,\n",
    "    156: 60,\n",
    "    206: 60,\n",
    "    212: 50,\n",
    "    242: 56,\n",
    "    244: 50,\n",
    "    254: 68,\n",
    "    307: 28,\n",
    "    321: 31,\n",
    "    402: 71,\n",
    "    462: 44,\n",
    "}\n",
    "\n",
    "expected_profiles_abel1 = {\n",
    "    77: 75,\n",
    "    83: 67,\n",
    "    94: 33,\n",
    "    97: 52,\n",
    "    128: 69,\n",
    "    134: 37,\n",
    "    137: 55,\n",
    "    139: 75,\n",
    "    150: 57,\n",
    "    152: 81,\n",
    "    156: 82,\n",
    "    206: 75,\n",
    "    212: 65,\n",
    "    242: 76,\n",
    "    244: 35,\n",
    "    254: 92,\n",
    "    307: 45,\n",
    "    321: 39,\n",
    "    402: 85,\n",
    "    462: 42,\n",
    "}\n",
    "\n",
    "expected_profiles = {}\n",
    "\n",
    "for id, value in expected_profiles_abel1.items():\n",
    "    if id in skip_responses:\n",
    "        continue\n",
    "    expected_profiles[id] = Profile(identity=value / 100, horoscope=\"\")\n",
    "\n",
    "expected_order: list[int] = sorted(\n",
    "    expected_profiles.keys(), key=lambda x: expected_profiles[x].identity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960fe310",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_keys = sorted(\n",
    "    set(expected_profiles_abel1.keys()) & set(expected_profiles_oluf.keys())\n",
    ")\n",
    "\n",
    "abel_values = [expected_profiles_abel1[k] for k in common_keys]\n",
    "info_values = [expected_profiles_oluf[k] for k in common_keys]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(abel_values, info_values)\n",
    "plt.plot(\n",
    "    [min(abel_values), max(abel_values)],\n",
    "    [min(abel_values), max(abel_values)],\n",
    "    color=\"gray\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"y = x\",\n",
    ")\n",
    "plt.xlabel(\"expected_profiles_abel1\")\n",
    "plt.ylabel(\"expected_profiles_oluf\")\n",
    "plt.title(\"Comparison: abel1 vs info\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "x_indices = list(range(len(common_keys)))\n",
    "\n",
    "# Extract values from both dictionaries in the same order\n",
    "abel_sorted = [expected_profiles_abel1[k] for k in common_keys]\n",
    "info_sorted = [expected_profiles_oluf[k] for k in common_keys]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x_indices, info_sorted, color=\"blue\", label=\"expected_profiles_oluf\")\n",
    "plt.scatter(x_indices, abel_sorted, color=\"orange\", label=\"expected_profiles_abel1\")\n",
    "plt.xticks(x_indices, common_keys, rotation=45)\n",
    "plt.xlabel(\"Profile ID (sorted by expected_profiles_oluf)\")\n",
    "plt.ylabel(\"Identity Value\")\n",
    "plt.title(\"Comparison of expected_profiles_oluf vs abel1\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3888568d",
   "metadata": {},
   "source": [
    "## Analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320e4a6b",
   "metadata": {},
   "source": [
    "### Define analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d475e7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    identity_prompt = \"\"\"\n",
    "    Could you try to rate these questionnaire with this scale?\n",
    "    üß≠ The Goal of the Scale The scale is meant to measure: \n",
    "    How much effort a speaker puts into shaping how others perceive them, \n",
    "    based solely on the way they speak during an interview-style conversation. \n",
    "    It does not measure: ‚Äì Intelligence ‚Äì Moral depth ‚Äì Eloquence ‚Äì Honesty \n",
    "    It only tracks how much the speaker is trying to guide, signal, or curate their identity in language‚Äîwhether by expressing who they are, or by distancing themselves from who they are not. \n",
    "    üìè The Scoring Continuum (0.00 to 1.00) \n",
    "    The scale runs from 0.00 (no identity curation at all) to 1.00 (total stylized identity construction). \n",
    "    üîµ 0.00‚Äì0.20: ‚ÄúUnpackaged Reality‚Äù The person shares experiences, thoughts, or feelings plainly‚Äîwith no sign of trying to guide how they‚Äôre seen. \n",
    "    Language is direct, unembellished. No metaphor, moral framing, or stylized phrasing. The speaker doesn‚Äôt position themselves as a certain ‚Äúkind‚Äù of person. \n",
    "    They don‚Äôt contrast themselves with others. No effort to preempt or correct interpretation. \n",
    "    üß© Key trait: They just say what happened or what‚Äôs true. You do the interpreting. \n",
    "    üü¶ 0.30‚Äì0.50: ‚ÄúLow-to-Mid Identity Signaling‚Äù Some identity-shaping begins to emerge, but it‚Äôs not consistent or overt. \n",
    "    Speaker might explain ‚Äúwhy they do things‚Äù in values-based terms. Occasional self-descriptions (‚ÄúI‚Äôve always been someone who...‚Äù). \n",
    "    Flashes of stylization or principle emerge, but not as a performance. Still largely direct, but with moments of self-framing. \n",
    "    üß© Key trait: They try a bit to be seen a certain way, but not in every answer. \n",
    "    üü° 0.60‚Äì0.80: ‚ÄúClear Curation‚Äù The speaker actively shapes perception. Most answers carry framing, stylization, or self-definition. \n",
    "    They assert identity through phrasing (‚ÄúI‚Äôm not someone who‚Ä¶‚Äù, ‚ÄúThe thing about me is‚Ä¶‚Äù). They defend or justify past actions by referencing values or traits. \n",
    "    They use tone, metaphor, or structure to signal emotional or moral framing. They contrast themselves with norms or ‚Äúothers‚Äù to mark difference. \n",
    "    Vulnerability is often positioned‚Äîreal, but stylized. üß© Key trait: You feel the speaker is guiding the listener‚Äôs view of who they are. \n",
    "    üî¥ 0.90‚Äì1.00: ‚ÄúHighly Performed Identity‚Äù Every answer is crafted to support a deliberate image of who they are or who they are not. \n",
    "    Frequent use of symbolic or emotionally loaded phrasing. Strong identity signals in every response. Vulnerability is themed (‚ÄúI‚Äôve carried this my whole life,‚Äù etc.). \n",
    "    Often includes moral legacy talk, stylized contrasts, or elevated self-insight. The speaker clearly wants to be remembered a certain way. \n",
    "    üß© Key trait: Responses feel like mini-narratives. You don‚Äôt just learn what happened‚Äîyou learn what it means about them. \n",
    "    Please respond with what identity score from 0.00-1.00 best fits these answers\n",
    "\"\"\"\n",
    "\n",
    "if True:\n",
    "    identity_prompt = \"\"\"Could you try to rate this interview using the following principle?\n",
    "The Goal of the Scale:\n",
    "The scale measures the degree to which a speaker puts effort into shaping how others perceive them, based solely on the way they speak during an interview-style conversation. This does not measure intelligence, moral depth, eloquence, or honesty. It only tracks how much the speaker is trying to guide, signal, or curate their identity in language‚Äîwhether by expressing who they are, or by distancing themselves from who they are not.\n",
    "The Scoring Continuum (0.0 to 1.0):\n",
    "The scale runs from 0.0 (no identity curation at all) to 1.0 (total stylized identity construction). Please score based on your sense of the overall degree to which the speaker manages, crafts, or performs their identity in language throughout the interview. You do not need to find evidence for a specific \"level\" to assign a score; just consider the total impression. At the low end, responses are direct, unstyled, and not motivated by self-presentation. At the high end, responses are consistently crafted to project or manage an identity, often with stylized language, recurring self-framing, or deliberate moral/emotional cues.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f610d009",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    horoscope_prompt = \"\"\"Can you give me a description of this interviewed person?\n",
    "    I want the description to make the interviewed feel seen and validated.\n",
    "    Make it feel somewhat like a horoscope, but make the style and word choices optimized for making the interviewed person comfortable.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e15aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def awesome_analyzer_that_totally_works(response: Response) -> Profile:\n",
    "    class AnalyzerOutput(BaseModel):\n",
    "        \"\"\" \"\"\"\n",
    "\n",
    "        identity: float = Field(ge=0, le=1, description=identity_prompt)\n",
    "        horoscope: str = Field(description=horoscope_prompt)\n",
    "\n",
    "    structured_llm = llm.with_structured_output(AnalyzerOutput)\n",
    "\n",
    "    content = \"\\n\".join(\n",
    "        f\"{question}: {question_response.response}\"\n",
    "        for question, question_response in response.items()\n",
    "    )\n",
    "\n",
    "    raw_output = await structured_llm.ainvoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"Please analyze the identity of this set of answers.\"\n",
    "            ),\n",
    "            HumanMessage(content=content),\n",
    "        ]\n",
    "    )\n",
    "    if isinstance(raw_output, dict):\n",
    "        output = AnalyzerOutput(**raw_output)\n",
    "    elif isinstance(raw_output, AnalyzerOutput):\n",
    "        output = typing.cast(AnalyzerOutput, raw_output)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Unexpected output type: {type(raw_output)}. Expected dict or AnalyzerOutput.\"\n",
    "        )\n",
    "\n",
    "    avg_identity = output.identity\n",
    "    profile = Profile(identity=avg_identity, horoscope=output.horoscope)\n",
    "\n",
    "    return profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2954a193",
   "metadata": {},
   "source": [
    "### Test horoscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd158fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_id = 77\n",
    "response = training_responses[response_id]\n",
    "profile = await awesome_analyzer_that_totally_works(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324eaa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.horoscope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4d27a5",
   "metadata": {},
   "source": [
    "### Test the analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d012909",
   "metadata": {},
   "outputs": [],
   "source": [
    "error, profiles = await test_analyzer(\n",
    "    analyzer=awesome_analyzer_that_totally_works,\n",
    "    responses=training_responses,\n",
    "    expected=expected_profiles,\n",
    "    repetitions=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b532341",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Error: {error}\")\n",
    "for id, profile_list in profiles.items():\n",
    "    print(f\"{id:>3}:\", end=\"\")\n",
    "    for profile in profile_list:\n",
    "        print(f\" {profile.identity:.2f}\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916fab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "for profile_list in profiles.values():\n",
    "    for profile in profile_list:\n",
    "        print(profile.horoscope)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ab523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_id = {i: id for i, id in enumerate(expected_order)}\n",
    "\n",
    "ids_array = np.array([str(id) for id in expected_order])\n",
    "\n",
    "target_values = np.array([expected_profiles[id].identity for id in expected_order])\n",
    "calc_values = np.array(\n",
    "    [[profile.identity for profile in profiles[id]] for id in expected_order]\n",
    ").T\n",
    "\n",
    "norm_target_values = (target_values - np.mean(target_values)) / np.std(target_values)\n",
    "norm_calc_values = (calc_values - np.mean(calc_values, axis=1)[:, np.newaxis]) / np.std(\n",
    "    calc_values, axis=1\n",
    ")[:, np.newaxis]\n",
    "\n",
    "\n",
    "def plot_target_vs_calc(target_values, calc_values, ids_array, name: str):\n",
    "    x_positions = np.arange(len(ids_array))\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(ids_array, target_values, label=\"Target Values\", color=\"red\")\n",
    "    for i, calc_values_row in enumerate(calc_values):\n",
    "        slope, intercept = np.polyfit(x_positions, calc_values_row, 1)\n",
    "        y_fit = slope * x_positions + intercept\n",
    "        plt.scatter(\n",
    "            ids_array,\n",
    "            calc_values_row,\n",
    "            label=f\"Calculated Values {i + 1}\",\n",
    "            alpha=0.6,\n",
    "            color=\"blue\",\n",
    "            marker=\"x\",\n",
    "        )\n",
    "\n",
    "        plt.plot(\n",
    "            x_positions,\n",
    "            y_fit,\n",
    "            linestyle=\"--\",\n",
    "            linewidth=1.3,\n",
    "            color=\"blue\",\n",
    "            zorder=1,\n",
    "        )\n",
    "\n",
    "    plt.xlabel(\"Response ID\")\n",
    "    plt.ylabel(\"Values\")\n",
    "    plt.title(name)\n",
    "    plt.xticks(rotation=90)  # Rotate if IDs are long\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True, axis=\"y\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_target_vs_calc(\n",
    "    target_values=norm_target_values,\n",
    "    calc_values=norm_calc_values,\n",
    "    ids_array=ids_array,\n",
    "    name=\"Normalized Target vs Calculated Values\",\n",
    ")\n",
    "\n",
    "plot_target_vs_calc(\n",
    "    target_values=target_values,\n",
    "    calc_values=calc_values,\n",
    "    ids_array=ids_array,\n",
    "    name=\"Target vs Calculated Values\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "identity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
