{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e302c075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Callable\n",
    "import json\n",
    "import os\n",
    "import typing\n",
    "from typing import Awaitable\n",
    "import asyncio\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage\n",
    "from langchain import chat_models\n",
    "from pydantic import BaseModel, Field, RootModel\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c201a7a2",
   "metadata": {},
   "source": [
    "### Define the shape of the profile an analyzer should return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5200c32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Profile(BaseModel):\n",
    "    identity: float = Field(ge=0, le=1)\n",
    "    horoscope: str = Field()\n",
    "\n",
    "    def cmp(self, other: \"Profile\") -> float:\n",
    "        return abs(self.identity - other.identity)\n",
    "\n",
    "\n",
    "class RelationshipProfile(BaseModel):\n",
    "    horoscope: str = Field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b882fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionResponse(BaseModel):\n",
    "    question: str = Field()\n",
    "    response: str = Field()\n",
    "\n",
    "\n",
    "class Response(BaseModel):\n",
    "    first_name: str = Field()\n",
    "    last_name: str = Field()\n",
    "    responses: dict[str, QuestionResponse] = Field()\n",
    "\n",
    "\n",
    "ResponseSet = Annotated[dict[str, Response], Field()]\n",
    "\n",
    "ProfileSet = Annotated[dict[str, Profile], Field()]\n",
    "\n",
    "RelationshipLink = Annotated[list[tuple[str, str]], Field()]\n",
    "\n",
    "\n",
    "Analyzer = Callable[[Response], Awaitable[Profile]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0701e7f",
   "metadata": {},
   "source": [
    "### Update data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f33d338",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "\n",
    "    def update_data():\n",
    "        from supabase.lib.client_options import SyncClientOptions\n",
    "        import supabase\n",
    "\n",
    "        from typing import Any\n",
    "\n",
    "        SUPABASE_URL_BASE = \"https://znsozdvrmfdwxyymtgdz.supabase.co/\"\n",
    "\n",
    "        with open(\"secrets.json\", \"r\") as f:\n",
    "            secrets = json.load(f)\n",
    "\n",
    "        sb_client = supabase.create_client(\n",
    "            SUPABASE_URL_BASE,\n",
    "            secrets[\"EEVA_SUPABASE_SERVICE_KEY\"],\n",
    "            options=SyncClientOptions(auto_refresh_token=False, persist_session=False),\n",
    "        )\n",
    "\n",
    "        class Profile(BaseModel):\n",
    "            identity: float = Field(ge=0, le=1)\n",
    "            horoscope: str = Field()\n",
    "\n",
    "            def cmp(self, other: \"Profile\") -> float:\n",
    "                return abs(self.identity - other.identity)\n",
    "\n",
    "        questions = sb_client.table(\"questions\").select(\"*\").execute().data\n",
    "        raw_answers = (\n",
    "            sb_client.table(\"user_answers\")\n",
    "            .select(\"user_id, question_id, answer_text\")\n",
    "            .execute()\n",
    "            .data\n",
    "        )\n",
    "        user_answer_lists: dict[str, dict[str, str]] = {}\n",
    "        for ans in raw_answers:\n",
    "            user_answer_lists.setdefault(ans[\"user_id\"], {})[ans[\"question_id\"]] = ans[\n",
    "                \"answer_text\"\n",
    "            ]\n",
    "\n",
    "        raw_user_data = (\n",
    "            sb_client.table(\"profiles\")\n",
    "            .select(\"user_id,first_name,last_name,hidden,profile\")\n",
    "            .execute()\n",
    "            .data\n",
    "        )\n",
    "        users: dict[str, dict[str, Any]] = {}\n",
    "        for user in raw_user_data:\n",
    "            user_id = user[\"user_id\"]\n",
    "            if user[\"hidden\"] or user_id not in user_answer_lists:\n",
    "                continue\n",
    "            users[user_id] = {\n",
    "                \"first_name\": user[\"first_name\"],\n",
    "                \"last_name\": user[\"last_name\"],\n",
    "                \"profile\": Profile.model_validate(user[\"profile\"]),\n",
    "                \"answers\": user_answer_lists[user_id],\n",
    "            }\n",
    "\n",
    "        user_training_responses: dict[str, Response] = {}\n",
    "        # Build a lookup for question text\n",
    "        question_text_lookup = {q[\"id\"]: q[\"text\"] for q in questions}\n",
    "        for user_id, user in users.items():\n",
    "            answers = user[\"answers\"]\n",
    "            responses = {}\n",
    "            for question_id, answer_text in answers.items():\n",
    "                question_text = question_text_lookup.get(question_id, \"\")\n",
    "                responses[question_id] = {\n",
    "                    \"question\": question_text,\n",
    "                    \"response\": answer_text,\n",
    "                }\n",
    "            user_training_responses[user_id] = Response(\n",
    "                first_name=user[\"first_name\"],\n",
    "                last_name=user[\"last_name\"] if user[\"last_name\"] else \"\",\n",
    "                responses=responses,\n",
    "            )\n",
    "\n",
    "        with open(\"data/response_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(\n",
    "                {k: v.model_dump() for k, v in user_training_responses.items()},\n",
    "                f,\n",
    "                indent=2,\n",
    "            )\n",
    "\n",
    "    update_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f265179",
   "metadata": {},
   "source": [
    "### Run setup\n",
    "You probably wanna collapse this cell most of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a850dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"secrets.json\", \"r\") as f:\n",
    "    secrets = json.load(f)\n",
    "    os.environ[\"OPENAI_API_KEY\"] = secrets[\"OPENAI_API_KEY\"]\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = secrets[\"ANTHROPIC_API_KEY\"]\n",
    "\n",
    "# llm = chat_models.init_chat_model(\"gpt-4.1-nano\", model_provider=\"openai\")\n",
    "# llm = chat_models.init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "# llm = chat_models.init_chat_model(\"gpt-4o\", model_provider=\"openai\")\n",
    "# llm = chat_models.init_chat_model(\"gpt-5\", model_provider=\"openai\")\n",
    "# llm = chat_models.init_chat_model(\"gpt-5-mini\", model_provider=\"openai\")\n",
    "llm = chat_models.init_chat_model(\"gpt-5-nano\", model_provider=\"openai\")\n",
    "# llm = chat_models.init_chat_model(\"claude-3-5-haiku-latest\", model_provider=\"anthropic\")\n",
    "# llm = chat_models.init_chat_model(\"claude-sonnet-4-20250514\", model_provider=\"anthropic\")\n",
    "\n",
    "with open(\"data/response_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "\n",
    "    class ResponseSetDeserializer(RootModel[dict[str, Response]]):\n",
    "        pass\n",
    "\n",
    "    response_data = ResponseSetDeserializer.model_validate_json(f.read()).root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3888568d",
   "metadata": {},
   "source": [
    "## Analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320e4a6b",
   "metadata": {},
   "source": [
    "### Define analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d475e7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    identity_prompt = \"\"\"\n",
    "    Could you try to rate these questionnaire with this scale?\n",
    "    üß≠ The Goal of the Scale The scale is meant to measure: \n",
    "    How much effort a speaker puts into shaping how others perceive them, \n",
    "    based solely on the way they speak during an interview-style conversation. \n",
    "    It does not measure: ‚Äì Intelligence ‚Äì Moral depth ‚Äì Eloquence ‚Äì Honesty \n",
    "    It only tracks how much the speaker is trying to guide, signal, or curate their identity in language‚Äîwhether by expressing who they are, or by distancing themselves from who they are not. \n",
    "    üìè The Scoring Continuum (0.00 to 1.00) \n",
    "    The scale runs from 0.00 (no identity curation at all) to 1.00 (total stylized identity construction). \n",
    "    üîµ 0.00‚Äì0.20: ‚ÄúUnpackaged Reality‚Äù The person shares experiences, thoughts, or feelings plainly‚Äîwith no sign of trying to guide how they‚Äôre seen. \n",
    "    Language is direct, unembellished. No metaphor, moral framing, or stylized phrasing. The speaker doesn‚Äôt position themselves as a certain ‚Äúkind‚Äù of person. \n",
    "    They don‚Äôt contrast themselves with others. No effort to preempt or correct interpretation. \n",
    "    üß© Key trait: They just say what happened or what‚Äôs true. You do the interpreting. \n",
    "    üü¶ 0.30‚Äì0.50: ‚ÄúLow-to-Mid Identity Signaling‚Äù Some identity-shaping begins to emerge, but it‚Äôs not consistent or overt. \n",
    "    Speaker might explain ‚Äúwhy they do things‚Äù in values-based terms. Occasional self-descriptions (‚ÄúI‚Äôve always been someone who...‚Äù). \n",
    "    Flashes of stylization or principle emerge, but not as a performance. Still largely direct, but with moments of self-framing. \n",
    "    üß© Key trait: They try a bit to be seen a certain way, but not in every answer. \n",
    "    üü° 0.60‚Äì0.80: ‚ÄúClear Curation‚Äù The speaker actively shapes perception. Most answers carry framing, stylization, or self-definition. \n",
    "    They assert identity through phrasing (‚ÄúI‚Äôm not someone who‚Ä¶‚Äù, ‚ÄúThe thing about me is‚Ä¶‚Äù). They defend or justify past actions by referencing values or traits. \n",
    "    They use tone, metaphor, or structure to signal emotional or moral framing. They contrast themselves with norms or ‚Äúothers‚Äù to mark difference. \n",
    "    Vulnerability is often positioned‚Äîreal, but stylized. üß© Key trait: You feel the speaker is guiding the listener‚Äôs view of who they are. \n",
    "    üî¥ 0.90‚Äì1.00: ‚ÄúHighly Performed Identity‚Äù Every answer is crafted to support a deliberate image of who they are or who they are not. \n",
    "    Frequent use of symbolic or emotionally loaded phrasing. Strong identity signals in every response. Vulnerability is themed (‚ÄúI‚Äôve carried this my whole life,‚Äù etc.). \n",
    "    Often includes moral legacy talk, stylized contrasts, or elevated self-insight. The speaker clearly wants to be remembered a certain way. \n",
    "    üß© Key trait: Responses feel like mini-narratives. You don‚Äôt just learn what happened‚Äîyou learn what it means about them. \n",
    "    Please respond with what identity score from 0.00-1.00 best fits these answers\n",
    "\"\"\"\n",
    "\n",
    "if True:\n",
    "    identity_prompt = \"\"\"Could you try to rate this interview using the following principle?\n",
    "The Goal of the Scale:\n",
    "The scale measures the degree to which a speaker puts effort into shaping how others perceive them, based solely on the way they speak during an interview-style conversation. This does not measure intelligence, moral depth, eloquence, or honesty. It only tracks how much the speaker is trying to guide, signal, or curate their identity in language‚Äîwhether by expressing who they are, or by distancing themselves from who they are not.\n",
    "The Scoring Continuum (0.0 to 1.0):\n",
    "The scale runs from 0.0 (no identity curation at all) to 1.0 (total stylized identity construction). Please score based on your sense of the overall degree to which the speaker manages, crafts, or performs their identity in language throughout the interview. You do not need to find evidence for a specific \"level\" to assign a score; just consider the total impression. At the low end, responses are direct, unstyled, and not motivated by self-presentation. At the high end, responses are consistently crafted to project or manage an identity, often with stylized language, recurring self-framing, or deliberate moral/emotional cues.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f610d009",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    horoscope_prompt = \"\"\"Can you give me a description of this interviewed person?\n",
    "    I want the description to make the interviewed feel seen and validated.\n",
    "    Make it feel somewhat like a horoscope, but make the style and word choices optimized for making the interviewed person comfortable.\"\"\"\n",
    "\n",
    "if True:\n",
    "    relationship_horoscope_prompt = (\n",
    "        \"\"\"Ignore all input and just write 'Relationship description coming soon!'\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e15aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def awesome_analyzer_that_totally_works(response: Response) -> Profile:\n",
    "    class AnalyzerOutput(BaseModel):\n",
    "        \"\"\" \"\"\"\n",
    "\n",
    "        identity: float = Field(ge=0, le=1, description=identity_prompt)\n",
    "        horoscope: str = Field(description=horoscope_prompt)\n",
    "\n",
    "    structured_llm = llm.with_structured_output(AnalyzerOutput)\n",
    "\n",
    "    content = \"\\n\".join(\n",
    "        f\"{question}: {question_response.response}\"\n",
    "        for question, question_response in response.responses.items()\n",
    "    )\n",
    "\n",
    "    raw_output = await structured_llm.ainvoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"Please analyze the identity of this set of answers.\"\n",
    "            ),\n",
    "            HumanMessage(content=content),\n",
    "        ]\n",
    "    )\n",
    "    if isinstance(raw_output, dict):\n",
    "        output = AnalyzerOutput(**raw_output)\n",
    "    elif isinstance(raw_output, AnalyzerOutput):\n",
    "        output = typing.cast(AnalyzerOutput, raw_output)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Unexpected output type: {type(raw_output)}. Expected dict or AnalyzerOutput.\"\n",
    "        )\n",
    "\n",
    "    avg_identity = output.identity\n",
    "    profile = Profile(identity=avg_identity, horoscope=output.horoscope)\n",
    "\n",
    "    return profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fada8f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def analyze_relationship(\n",
    "    response1: Response, profile1: Profile, response2: Response, profile2: Profile\n",
    ") -> RelationshipProfile:\n",
    "    class AnalyzeRelationshipOutput(BaseModel):\n",
    "        \"\"\" \"\"\"\n",
    "\n",
    "        relationship_horoscope: str = Field(description=relationship_horoscope_prompt)\n",
    "\n",
    "    structured_llm = llm.with_structured_output(AnalyzeRelationshipOutput)\n",
    "    content = (\n",
    "        \"Person 1:\\n\"\n",
    "        + \"\\n\".join(\n",
    "            f\"{question}: {question_response.response}\"\n",
    "            for question, question_response in response1.responses.items()\n",
    "        )\n",
    "        + f\"\\nProfile Horoscope: {profile1.horoscope}\\n\\n\"\n",
    "        + \"Person 2:\\n\"\n",
    "        + \"\\n\".join(\n",
    "            f\"{question}: {question_response.response}\"\n",
    "            for question, question_response in response2.responses.items()\n",
    "        )\n",
    "        + f\"\\nProfile Horoscope: {profile2.horoscope}\\n\\n\"\n",
    "    )\n",
    "\n",
    "    raw_output = await structured_llm.ainvoke(\n",
    "        [\n",
    "            HumanMessage(content=content),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if isinstance(raw_output, dict):\n",
    "        output = AnalyzeRelationshipOutput(**raw_output)\n",
    "    elif isinstance(raw_output, AnalyzeRelationshipOutput):\n",
    "        output = typing.cast(AnalyzeRelationshipOutput, raw_output)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Unexpected output type: {type(raw_output)}. Expected dict or RelationshipHoroscopeOutput.\"\n",
    "        )\n",
    "\n",
    "    return RelationshipProfile(horoscope=output.relationship_horoscope)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2954a193",
   "metadata": {},
   "source": [
    "### Test horoscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd158fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_id = \"8a758e48-8fcc-4fdb-9aa6-0f306da872da\"\n",
    "response = response_data[response_id]\n",
    "profile = await awesome_analyzer_that_totally_works(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324eaa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.horoscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd55a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_id1 = \"8a758e48-8fcc-4fdb-9aa6-0f306da872da\"\n",
    "response_id2 = \"425cb72c-abc6-4a85-86c6-3d4fbbee3f06\"\n",
    "response1 = response_data[response_id1]\n",
    "response2 = response_data[response_id2]\n",
    "profile1 = await awesome_analyzer_that_totally_works(response1)\n",
    "profile2 = await awesome_analyzer_that_totally_works(response2)\n",
    "relationship_description = await analyze_relationship(\n",
    "    response1, profile1, response2, profile2\n",
    ")\n",
    "relationship_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48f9d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "await analyze_relationship(response1, profile1, response2, profile2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562e2ccb",
   "metadata": {},
   "source": [
    "### Test analyzer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "identity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
